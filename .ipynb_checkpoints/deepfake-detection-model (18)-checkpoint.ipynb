{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13105087,"sourceType":"datasetVersion","datasetId":8301318},{"sourceId":13105799,"sourceType":"datasetVersion","datasetId":8301820}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import logging\nimport os\nimport random\nimport sys\nfrom kaggle_secrets import UserSecretsClient\nimport webdav.client as wc\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport multiprocessing\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom facenet_pytorch import MTCNN\nfrom PIL import Image\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","editable":true,"slideshow":{"slide_type":""},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:43.592404Z","iopub.execute_input":"2025-10-05T04:37:43.592711Z","iopub.status.idle":"2025-10-05T04:37:43.598485Z","shell.execute_reply.started":"2025-10-05T04:37:43.592693Z","shell.execute_reply":"2025-10-05T04:37:43.597650Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"BASE_VIDEO_PATH = \"/kaggle/input\"\nMETADATA_PATH = \"/kaggle/input/metadata/celeb_df_metadata.csv\"\n\n# Access the secrets for Nextcloud authentication\nuser_secrets = UserSecretsClient()\nNC_USER = user_secrets.get_secret(\"NC_USER\")\nNC_PASSWORD = user_secrets.get_secret(\"NC_PASSWORD\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:43.599641Z","iopub.execute_input":"2025-10-05T04:37:43.600156Z","iopub.status.idle":"2025-10-05T04:37:44.180791Z","shell.execute_reply.started":"2025-10-05T04:37:43.600138Z","shell.execute_reply":"2025-10-05T04:37:44.180212Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"# Data Preprocessing Pipeline","metadata":{}},{"cell_type":"markdown","source":"Imports and Logging Setup: This section imports necessary libraries (e.g., cv2 for video handling, torch for tensors, MTCNN for face detection) and configures logging. Logging is crucial for robustness, as the PDF highlights potential issues like corrupted videos or face detection failures; it allows tracking warnings/errors without crashing the pipeline.\n\nClass Initialization (__init__): Initializes the dataset with metadata, transformations, frame count, and image size. It sets up MTCNN for face detection on GPU if available, ensuring efficiency for large datasets like Celeb-DF-v2 with varying video qualities.\n\nLength Method (__len__): Returns the number of videos in the metadata, enabling PyTorch's DataLoader to iterate correctly. This is standard but essential for handling the imbalanced dataset size (890 real vs. 5639 fake).\n\nFace Extraction Method (extract_faces_from_video): Handles video reading with error checks (e.g., file existence, opening failures) to prevent crashes on problematic files. It uses adaptive sampling (random for short videos, uniform for longer ones) to capture temporal information without bias, extracts faces via MTCNN, and logs failures for debugging.\n\nItem Retrieval Method (__getitem__): Fetches a video's data by index, assigns binary labels (0 for real, 1 for fake), extracts faces, and handles shortages with noise-added repetitions to avoid overfitting on duplicates. It applies transformations and stacks frames into a tensor, preparing consistent inputs despite dataset variability.","metadata":{}},{"cell_type":"code","source":"# Setup logging for robustness\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n# Reconfigure the root logger to ensure output appears in the notebook\nlogger = logging.getLogger()\nlogger.handlers = []  # Clear any existing handlers\nhandler = logging.StreamHandler(sys.stdout)  # Create a new handler to stream to the console\nformatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\nnextcloud_options = {\n    'webdav_hostname': \"https://nextcloud.lukaxzs.myaddr.io\",\n    'webdav_login':    NC_USER,\n    'webdav_password': NC_PASSWORD,\n    'webdav_username': NC_USER # Required for building the file path\n}\n\nclass DeepfakeDataset(Dataset):\n\n    def __init__(\n        self, metadata_df, transform=None, frames_per_video=30, image_size=224, local_cache_dir='/kaggle/working/temp_face_cache', nextcloud_options=None\n    ):\n        self.metadata = metadata_df\n        self.transform = transform\n        self.frames_per_video = frames_per_video\n        self.image_size = image_size\n        self.local_cache_dir = local_cache_dir\n        self.nextcloud_options = nextcloud_options\n        self.nc = None # To hold the Nextcloud client connection\n        \n        self.mtcnn = MTCNN(\n            image_size=image_size,\n            margin=20,\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        )\n        # Create the cache directory if it doesn't exist\n        if not os.path.exists(self.local_cache_dir):\n            os.makedirs(self.local_cache_dir)\n            logging.info(f\"Created cache directory at: {self.local_cache_dir}\")\n\n        # Connect to Nextcloud if options are provided\n        if self.nextcloud_options:\n            try:\n                self.nc = wc.Client(self.nextcloud_options)\n                self.nc.verify = True # Ensures SSL certificate is verified\n                logging.info(\"Successfully connected to Nextcloud server.\")\n            except Exception as e:\n                logging.error(f\"Failed to connect to Nextcloud: {e}\")\n                self.nc = None\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def extract_faces_from_video(self, video_path):\n        \"\"\"Extract faces from video frames using MTCNN with error handling\"\"\"\n        if not os.path.exists(video_path):\n            logging.warning(f\"Video file not found: {video_path}\")\n            return []\n\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            logging.error(f\"Failed to open video: {video_path}\")\n            return []\n\n        faces = []\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Adaptive sampling: Adjust based on video length to avoid over-sampling short videos\n        if frame_count < self.frames_per_video:\n            frame_indices = np.arange(frame_count)\n            np.random.shuffle(frame_indices)  # Randomize for short videos\n        else:\n            frame_indices = np.linspace(\n                0, frame_count - 1, self.frames_per_video, dtype=int\n            )\n\n        for frame_idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n            ret, frame = cap.read()\n\n            if ret:\n                try:\n                    # Convert BGR to RGB\n                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    pil_image = Image.fromarray(frame_rgb)\n\n                    # Extract face using MTCNN with fallback\n                    face = self.mtcnn(pil_image)\n                    if face is None:\n                        # Retry with adjusted parameters\n                        self.mtcnn.margin = 40  # Increase margin temporarily\n                        face = self.mtcnn(pil_image)\n                        self.mtcnn.margin = 20  # Reset\n\n                    if face is not None:\n                        faces.append(face)\n                except Exception as e:\n                    logging.warning(\n                        f\"Face extraction failed for frame {frame_idx} in {video_path}: {str(e)}\"\n                    )\n\n        cap.release()\n        if len(faces) > 0:\n            logging.info(f\"Successfully extracted {len(faces)} faces from {video_path}\")\n        return faces\n\n    def __getitem__(self, idx):\n        row = self.metadata.iloc[idx]\n        video_path = row['filepath']\n        label = 1 if row['label'] == 'fake' else 0\n        \n        # Define local and remote paths for the cached tensor\n        cache_filename = os.path.basename(video_path).replace('.mp4', '.pt')\n        local_cache_path = os.path.join(self.local_cache_dir, cache_filename)\n        remote_cache_path = f\"/Backup/DeepfakeDetection/{cache_filename}\"\n        \n        faces = None\n        \n        # 1. ALWAYS check Nextcloud first. This is now the primary source of truth.\n        if self.nc and self.nc.check(remote_cache_path):\n            try:\n                logging.info(f\"Cache found on Nextcloud. Downloading: {remote_cache_path}\")\n                # Download from Nextcloud to the local temporary directory\n                self.nc.download_file(remote_cache_path, local_cache_path)\n                # Load the faces from the freshly downloaded file\n                faces = torch.load(local_cache_path)\n                logging.info(f\"Successfully loaded faces from Nextcloud download.\")\n                # IMPORTANT: Clean up the local file immediately after loading to save space\n                os.remove(local_cache_path)\n            except Exception as e:\n                logging.error(f\"Failed to download or load from Nextcloud: {e}\")\n                # Proceed as if no cache exists\n        \n        # 2. If no cache was found on Nextcloud, then (and only then) extract the faces.\n        if faces is None:\n            logging.info(f\"No remote cache for {video_path}. Extracting faces...\")\n            extracted_faces = self.extract_faces_from_video(video_path)\n            \n            if extracted_faces and len(extracted_faces) > 0:\n                faces = extracted_faces\n                # Save to the local temp file to prepare for upload\n                torch.save(faces, local_cache_path)\n                \n                if self.nc:\n                    try:\n                        logging.info(f\"Uploading to Nextcloud: {remote_cache_path}\")\n                        # --- CRITICAL FIX HERE ---\n                        # The first argument is the LOCAL source, the second is the REMOTE destination.\n                        self.nc.upload_file(local_cache_path, remote_cache_path)\n                        logging.info(\"Upload successful.\")\n                        # Clean up the local temp file after a successful upload\n                        os.remove(local_cache_path)\n                        logging.info(\"Removed local temp file after upload.\")\n                    except Exception as e:\n                        logging.error(f\"Failed to upload to Nextcloud: {e}\")\n            else:\n                faces = []\n    \n        # --- From here, the original logic proceeds ---\n    \n        if len(faces) == 0:\n            logging.warning(f\"No faces available for {video_path}. Skipping this sample.\")\n            return None, None\n    \n        # Handle cases where fewer faces are extracted than needed\n        if len(faces) < self.frames_per_video:\n            while len(faces) < self.frames_per_video:\n                faces.append(faces[-1])  # Duplicate the last available face\n    \n        # Trim if more faces are extracted than needed\n        if len(faces) > self.frames_per_video:\n            faces = faces[:self.frames_per_video]\n    \n        # Stack faces into a single tensor\n        faces_tensor = torch.stack(faces)\n    \n        # Apply transformations\n        if self.transform:\n            transformed_faces = []\n            for face in faces_tensor:\n                transformed = self.transform(face)\n                if torch.isnan(transformed).any() or torch.isinf(transformed).any():\n                    logging.warning(f\"NaN/Inf in transformed face for {vide_opath}, skipping sample.\")\n                    return None, None\n                transformed_faces.append(transformed)\n            faces_tensor = torch.stack(transformed_faces)\n    \n        return faces_tensor, torch.tensor(label, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:44.181878Z","iopub.execute_input":"2025-10-05T04:37:44.182089Z","iopub.status.idle":"2025-10-05T04:37:44.199355Z","shell.execute_reply.started":"2025-10-05T04:37:44.182073Z","shell.execute_reply":"2025-10-05T04:37:44.198634Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Data Augmentation and Transforms","metadata":{}},{"cell_type":"markdown","source":"Training Transforms (train_transforms): Composes a sequence of augmentations starting with PIL conversion for compatibility, resizing to 224x224 (standard for backbones like EfficientNet), and random flips/rotations to simulate pose variations. Color jitter and affine shear address lighting/quality inconsistencies, enhancing robustness without masking deepfake artifacts; normalization uses ImageNet stats for transfer learning.\n\nValidation Transforms (val_transforms): A minimal composition for evaluation, including PIL conversion, resizing, tensor conversion, and normalization. This ensures consistent inputs without random augmentations, allowing fair assessment of model generalization on the imbalanced validation set.","metadata":{}},{"cell_type":"code","source":"# Define enhanced data augmentation for training robustness, considering dataset variability\ntrain_transforms = transforms.Compose(\n    [\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=15),  # Increased range for pose variation\n        transforms.ColorJitter(\n            brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2\n        ),  # Stronger jitter for lighting inconsistencies\n        transforms.RandomAffine(\n            degrees=0, shear=10\n        ),  # Add shear for facial distortion artifacts\n        transforms.GaussianBlur(\n            kernel_size=3, sigma=(0.1, 2.0)\n        ),  # Simulates video artifacts.\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\nval_transforms = transforms.Compose(\n    [\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:44.199922Z","iopub.execute_input":"2025-10-05T04:37:44.200151Z","iopub.status.idle":"2025-10-05T04:37:44.219888Z","shell.execute_reply.started":"2025-10-05T04:37:44.200135Z","shell.execute_reply":"2025-10-05T04:37:44.219108Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Model Architecture Implementation","metadata":{}},{"cell_type":"markdown","source":"Class Initialization (__init__): Sets up the model with frame count (20 for better temporal coverage), selects a pretrained CNN backbone (e.g., EfficientNet-B4 for efficiency on facial details), and defines bidirectional LSTMs for temporal analysis, attention layers for focusing on key frames, and a classifier with batch norm/dropout for stability on imbalanced data.\n\nForward Pass (forward): Reshapes input for batch CNN processing, extracts features with mixed precision for speed, reshapes for LSTM, applies bidirectional temporal modeling to detect inconsistencies, weights frames via attention (useful for variable-length videos), and classifies via dense layers, outputting a sigmoid probability for real/fake.","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\nfrom torch.cuda.amp import autocast\nfrom torch.nn import functional as F\n\n\nclass DeepfakeDetector(nn.Module):\n    def __init__(self, num_frames=20, backbone=\"efficientnet_b4\", dropout_rate=0.6):\n        super(DeepfakeDetector, self).__init__()\n        self.num_frames = num_frames\n\n        # CNN Backbone for feature extraction\n        if backbone == \"efficientnet_b4\":\n            weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n            # self.backbone = models.efficientnet_b4(pretrained=True)\n            self.backbone = models.efficientnet_b4(weights=weights)\n            self.backbone.classifier = nn.Identity()  # Remove final classifier\n            feature_dim = 1792\n        elif backbone == \"resnet50\":\n            weights = models.ResNet50_Weights.IMAGENET1K_V1\n            self.backbone = models.resnet50(weights=weights)\n            # self.backbone = models.resnet50(pretrained=True)\n            self.backbone.fc = nn.Identity()\n            feature_dim = 2048\n\n        # Temporal processing layers with bidirectional LSTM for better sequence modeling\n        self.lstm = nn.LSTM(\n            input_size=feature_dim,\n            hidden_size=512,\n            num_layers=3,\n            batch_first=True,\n            dropout=dropout_rate,\n            bidirectional=True,\n        )\n\n        # Attention mechanism for frame importance weighting\n        self.attention = nn.Sequential(\n            nn.Linear(1024, 256), nn.ReLU(), nn.Linear(256, 1), nn.Softmax(dim=1)\n        )\n\n        # Final classification layers with additional regularization\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),  # Adjusted for bidirectional\n            nn.BatchNorm1d(256),  # Added for stability\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(128, 1),\n            # nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        batch_size, num_frames, C, H, W = x.shape\n        x = x.view(batch_size * num_frames, C, H, W)\n\n        # Use torch.amp.autocast for mixed precision\n        with torch.amp.autocast(device_type=\"cuda\"):\n            features = self.backbone(x)\n\n        features = features.view(batch_size, num_frames, -1)\n        lstm_out, _ = self.lstm(features)\n\n        attention_weights = self.attention(lstm_out)\n\n        # The correct approach is to multiply lstm_out by the attention weights\n        # and then sum across the time dimension.\n        # weighted_features = (lstm_out * attention_weights).sum(dim=1)\n        # weighted_features = lstm_out * attention_weights + lstm_out  # Helps with gradient flow.\n        # weighted_features = weighted_features.mean(dim=1)  # Reduce over sequence dimension (average pooling)\n\n        # output = self.classifier(weighted_features)\n        context_vector = (lstm_out * attention_weights).sum(dim=1)\n        output = self.classifier(context_vector)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:44.221504Z","iopub.execute_input":"2025-10-05T04:37:44.221727Z","iopub.status.idle":"2025-10-05T04:37:44.237531Z","shell.execute_reply.started":"2025-10-05T04:37:44.221713Z","shell.execute_reply":"2025-10-05T04:37:44.237005Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# Training Setup and Configuration","metadata":{}},{"cell_type":"markdown","source":"Training Setup Function (setup_training): Loads metadata, performs stratified split to preserve imbalance ratios, creates datasets, computes sample weights for oversampling (addressing PDF's 1:6 imbalance), and sets up DataLoaders with a sampler for balanced batching.\n\nTraining Function (train_model): Configures device/loss/optimizer with weighted BCE for imbalance, initializes mixed-precision scaler, and runs epochs with training loops (forward pass, backprop with clipping for stability), validation (no-grad inference), metrics calculation, scheduling, and early stopping to save the best model and prevent overfitting.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch.amp\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        bce_loss = nn.BCEWithLogitsLoss()(inputs, targets)\n        pt = torch.exp(-bce_loss)\n        return self.alpha * (1 - pt) ** self.gamma * bce_loss\n\n\n# filters out any None values returned by the dataset\ndef collate_fn(batch):\n    # Filter out samples that are None\n    batch = [b for b in batch if b[0] is not None]\n    if not batch:\n        # Return empty tensors if the whole batch is invalid\n        return torch.tensor([]), torch.tensor([])\n    return torch.utils.data.dataloader.default_collate(batch)\n\n\ndef setup_training():\n    # Load metadata from dataset analysis\n    metadata = pd.read_csv(METADATA_PATH)\n    # metadata['filepath'] = metadata['filepath'].apply(lambda x: os.path.join(BASE_VIDEO_PATH, x))\n    metadata[\"filepath\"] = metadata[\"filepath\"].apply(\n        # lambda x: os.path.join(BASE_VIDEO_PATH, x)\n        lambda x: os.path.join(BASE_VIDEO_PATH, x.replace('Celeb-DF-v2', 'celeb-df-v2', 1))\n    )\n\n    # Train/validation split with stratification\n    train_df, val_df = train_test_split(\n        metadata, test_size=0.2, stratify=metadata[\"label\"], random_state=42\n    )\n\n    # Define a cache directory in a writable location\n    face_cache_directory = '/kaggle/working/face_cache'\n    # Create datasets\n    train_dataset = DeepfakeDataset(train_df, transform=train_transforms, nextcloud_options=nextcloud_options)\n    val_dataset = DeepfakeDataset(val_df, transform=val_transforms, nextcloud_options=nextcloud_options)\n\n    # Compute class weights for oversampling to handle imbalance (real: ~890, fake: ~5639 total)\n    class_counts = metadata[\"label\"].value_counts()\n    # class_weights = {0: 1.0 / class_counts['real'], 1: 1.0 / class_counts['fake']}\n    class_weights = {\n        0: class_counts[\"fake\"] / class_counts[\"real\"],\n        1: 1.0,\n    }  # This oversamples real videos more heavily.\n\n    # sample_weights = [class_weights[0] if label == 'real' else class_weights[1] for label in train_df['label']]\n    # sample_weights = [class_weights[label] for label in train_df['label']]\n    sample_weights = [\n        class_weights[1 if label == \"fake\" else 0] for label in train_df[\"label\"]\n    ]\n\n    sampler = WeightedRandomSampler(\n        sample_weights, len(sample_weights), replacement=True\n    )\n\n    # Create data loaders with oversampling\n    # Apply the collate_fn to both training and validation loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=10,\n        sampler=sampler,\n        num_workers=2,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=10,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )\n\n    return train_loader, val_loader\n\n\n# Training function with early stopping and gradient clipping\ndef train_model(\n    model, train_loader, val_loader, num_epochs=30, learning_rate=1e-5, patience=10\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Use BCEWithLogitsLoss for numerical stability\n    # criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=learning_rate, weight_decay=1e-4\n    )\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=3, factor=0.5\n    )\n\n    # Initialize GradScaler for mixed precision\n    scaler = GradScaler()\n\n    # Class-weighted loss for imbalance (weight fakes lower since more prevalent)\n    pos_weight = torch.tensor(\n        [\n            len(train_loader.dataset)\n            / (2 * sum(target == 1 for _, target in train_loader.dataset))\n        ]\n    )\n    # criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))  # Use logits version for stability\n    criterion = FocalLoss().to(device)\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=learning_rate, weight_decay=1e-4\n    )\n    scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n    scaler = GradScaler()  # For mixed precision\n\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss, train_correct = 0.0, 0\n\n        for batch_idx, (data, targets) in enumerate(train_loader):\n            if data.nelement() == 0:  # Skip empty batches\n                continue\n            data, targets = data.to(device), targets.to(device)\n            targets = targets.unsqueeze(1)\n\n            optimizer.zero_grad()\n\n            # Use torch.amp.autocast\n            with torch.amp.autocast(device_type=\"cuda\"):\n                outputs = model(data)\n                loss = criterion(outputs, targets)\n\n            scaler.scale(loss).backward()\n            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n\n            scaler.unscale_(optimizer)  # Unscale gradients before clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item()\n            predictions = (torch.sigmoid(outputs) > 0.5).float()\n            train_correct += (predictions == targets).sum().item()\n\n            if batch_idx % 10 == 0:\n                logging.info(\n                    f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\"\n                )\n\n        # Validation phase\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        all_predictions, all_targets = [], []\n\n        with torch.no_grad(), autocast():\n            for data, targets in val_loader:\n                data, targets = data.to(device), targets.to(device)\n                targets = targets.unsqueeze(1)\n\n                outputs = model(data)\n                loss = criterion(outputs, targets)\n\n                probabilities = torch.sigmoid(outputs)\n                predictions = (probabilities > 0.5).float()\n\n                val_loss += loss.item()\n                predictions = (torch.sigmoid(outputs) > 0.5).float()\n                val_correct += (predictions == targets).sum().item()\n\n                all_predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n                all_targets.extend(targets.cpu().numpy())\n\n        # Calculate metrics\n        train_acc = train_correct / len(train_loader.dataset)\n        val_acc = val_correct / len(val_loader.dataset)\n        auc_score = roc_auc_score(all_targets, all_predictions)\n        avg_val_loss = val_loss / len(val_loader)\n\n        train_losses.append(train_loss / len(train_loader))\n        val_losses.append(avg_val_loss)\n        train_accs.append(train_acc)\n        val_accs.append(val_acc)\n\n        scheduler.step(avg_val_loss)\n\n        logging.info(\n            f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, AUC: {auc_score:.4f}\"\n        )\n\n        # Early stopping\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), \"best_deepfake_detector_model.pth\")\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                logging.info(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    return train_losses, val_losses, train_accs, val_accs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:44.238210Z","iopub.execute_input":"2025-10-05T04:37:44.238394Z","iopub.status.idle":"2025-10-05T04:37:44.258065Z","shell.execute_reply.started":"2025-10-05T04:37:44.238381Z","shell.execute_reply":"2025-10-05T04:37:44.257591Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"# Model Validation and Evaluation","metadata":{}},{"cell_type":"markdown","source":"Evaluation Function (evaluate_model): Sets model to eval mode, collects predictions/probabilities with no-grad and mixed precision, computes metrics like accuracy/AUC/PR curves, and calculates EER (key for imbalanced security tasks per PDF). It generates and saves plots for visual analysis.\n\nMain Execution Block (if __name__ == \"__main__\"): Orchestrates the pipeline by setting up loaders, initializing the model, training, evaluating, and saving the final model, with logging for progress tracking.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\n\ndef evaluate_model(model, val_loader):\n    \"\"\"Comprehensive model evaluation with multiple metrics including EER\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n\n    all_predictions = []\n    all_probabilities = []\n    all_targets = []\n\n    with torch.no_grad(), autocast():\n        for data, targets in val_loader:\n            data, targets = data.to(device), targets.to(device)\n            targets = targets.unsqueeze(1)\n\n            outputs = model(data)  # Raw logits\n\n            probabilities = torch.sigmoid(outputs)\n            predictions = (probabilities > 0.5).float()\n\n            all_predictions.extend(predictions.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    # Calculate comprehensive metrics\n    accuracy = accuracy_score(all_targets, all_predictions)\n    auc_score = roc_auc_score(all_targets, all_probabilities)\n\n    # Precision-Recall curve\n    precision, recall, thresholds = precision_recall_curve(\n        all_targets, all_probabilities\n    )\n\n    # Equal Error Rate (EER) calculation\n    fpr, tpr, thresh = roc_curve(all_targets, all_probabilities)\n    fnr = 1 - tpr\n    eer_threshold = thresh[np.nanargmin(np.absolute(fnr - fpr))]\n    if np.all(fpr == 0) or np.all(fnr == 0):\n        eer = 0.5  # Fallback for degenerate cases\n    else:\n        eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n\n    # Plot evaluation metrics\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n    # ROC Curve\n    axes[0].plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.3f})\")\n    axes[0].plot([0, 1], [0, 1], \"k--\")\n    axes[0].set_xlabel(\"False Positive Rate\")\n    axes[0].set_ylabel(\"True Positive Rate\")\n    axes[0].set_title(\"ROC Curve\")\n    axes[0].legend()\n\n    # Precision-Recall Curve\n    axes[1].plot(recall, precision, label=f\"PR Curve\")\n    axes[1].set_xlabel(\"Recall\")\n    axes[1].set_ylabel(\"Precision\")\n    axes[1].set_title(\"Precision-Recall Curve\")\n    axes[1].legend()\n\n    plt.tight_layout()\n    plt.savefig(\"model_evaluation_metrics.png\", dpi=300, bbox_inches=\"tight\")\n    plt.show()\n\n    print(f\"Model Evaluation Results:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"AUC-ROC: {auc_score:.4f}\")\n    print(f\"Equal Error Rate (EER): {eer:.4f}\")\n\n    return accuracy, auc_score, eer\n\n\n# Main execution code\nif __name__ == \"__main__\":\n    # Explicitly tells CUDA to use the 'spawn' start method\n    multiprocessing.set_start_method(\"spawn\", force=True)\n\n    logging.info(\"Initializing deepfake detection model...\")\n\n    # Setup data loaders\n    train_loader, val_loader = setup_training()\n\n    # Initialize model\n    model = DeepfakeDetector(num_frames=20, backbone=\"efficientnet_b4\")\n\n    logging.info(\"Starting model training...\")\n    train_losses, val_losses, train_accs, val_accs = train_model(\n        model, train_loader, val_loader, num_epochs=20, patience=5\n    )\n    # Plot the training and validation loss\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training and Validation Loss Over Epochs\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(\"loss_curves.png\")\n    plt.show()\n\n    logging.info(\"Evaluating model performance...\")\n    accuracy, auc_score, eer = evaluate_model(model, val_loader)\n\n    # Save final model\n    torch.save(model.state_dict(), \"deepfake_detector_model.pth\")\n    logging.info(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T04:37:44.258733Z","iopub.execute_input":"2025-10-05T04:37:44.258949Z","iopub.status.idle":"2025-10-05T04:37:55.301387Z","shell.execute_reply.started":"2025-10-05T04:37:44.258935Z","shell.execute_reply":"2025-10-05T04:37:55.300401Z"}},"outputs":[{"name":"stdout","text":"2025-10-05 04:37:44,274 - INFO - Initializing deepfake detection model...\n2025-10-05 04:37:44,320 - INFO - Successfully connected to Nextcloud server.\n2025-10-05 04:37:44,346 - INFO - Successfully connected to Nextcloud server.\n2025-10-05 04:37:44,910 - INFO - Starting model training...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2854693623.py:108: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"2025-10-05 04:37:46,163 - INFO - No remote cache for /kaggle/input/celeb-df-v2/Celeb-synthesis/id38_id31_0000.mp4. Extracting faces...\n2025-10-05 04:37:48,033 - INFO - Successfully extracted 30 faces from /kaggle/input/celeb-df-v2/Celeb-synthesis/id38_id31_0000.mp4\n2025-10-05 04:37:48,074 - INFO - Uploading to Nextcloud: /Backup/DeepfakeDetection/id38_id31_0000.pt\n2025-10-05 04:37:48,074 - ERROR - Failed to upload to Nextcloud: Local file: /Backup/DeepfakeDetection/id38_id31_0000.pt not found\n2025-10-05 04:37:48,618 - INFO - No remote cache for /kaggle/input/celeb-df-v2/Celeb-synthesis/id19_id25_0001.mp4. Extracting faces...\n2025-10-05 04:37:50,947 - INFO - Successfully extracted 30 faces from /kaggle/input/celeb-df-v2/Celeb-synthesis/id19_id25_0001.mp4\n2025-10-05 04:37:50,983 - INFO - Uploading to Nextcloud: /Backup/DeepfakeDetection/id19_id25_0001.pt\n2025-10-05 04:37:50,984 - ERROR - Failed to upload to Nextcloud: Local file: /Backup/DeepfakeDetection/id19_id25_0001.pt not found\n2025-10-05 04:37:51,495 - INFO - No remote cache for /kaggle/input/celeb-df-v2/Celeb-synthesis/id33_id23_0001.mp4. Extracting faces...\n2025-10-05 04:37:53,154 - INFO - Successfully extracted 30 faces from /kaggle/input/celeb-df-v2/Celeb-synthesis/id33_id23_0001.mp4\n2025-10-05 04:37:53,190 - INFO - Uploading to Nextcloud: /Backup/DeepfakeDetection/id33_id23_0001.pt\n2025-10-05 04:37:53,191 - ERROR - Failed to upload to Nextcloud: Local file: /Backup/DeepfakeDetection/id33_id23_0001.pt not found\n2025-10-05 04:37:53,724 - INFO - No remote cache for /kaggle/input/celeb-df-v2/Celeb-synthesis/id32_id35_0007.mp4. Extracting faces...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2421234253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     train_losses, val_losses, train_accs, val_accs = train_model(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     )\n","\u001b[0;32m/tmp/ipykernel_36/2854693623.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, patience)\u001b[0m\n\u001b[1;32m    112\u001b[0m         [\n\u001b[1;32m    113\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         ]\n\u001b[1;32m    116\u001b[0m     )\n","\u001b[0;32m/tmp/ipykernel_36/2854693623.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m         [\n\u001b[1;32m    113\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         ]\n\u001b[1;32m    116\u001b[0m     )\n","\u001b[0;32m/tmp/ipykernel_36/1315256484.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No remote cache for {video_path}. Extracting faces...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mextracted_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_faces_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextracted_faces\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_faces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1315256484.py\u001b[0m in \u001b[0;36mextract_faces_from_video\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0;31m# Extract face using MTCNN with fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpil_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                         \u001b[0;31m# Retry with adjusted parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m                         raise TypeError(f\"cannot assign '{torch.typename(value)}' as buffer '{name}' \"\n\u001b[0;32m-> 1739\u001b[0;31m                                         \u001b[0;34m\"(torch.Tensor or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m                                         )\n\u001b[1;32m   1741\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_global_buffer_registration_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1750\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/facenet_pytorch/models/mtcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Detect faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mbatch_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# Select faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/facenet_pytorch/models/mtcnn.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \"\"\"\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             batch_boxes, batch_points = detect_face(\n\u001b[1;32m    314\u001b[0m                 \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_face_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":38}]}