{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a978853e-75b3-412f-a4a9-b8b1e19d7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Setup logging to display progress and informational messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792d72f-a5b6-4fda-9a7d-fb345d996a20",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "To load the saved model weights, we must first define the model's architecture. This class must be identical to the `DeepfakeDetector` class used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfa5319-34c6-4789-b816-8e5615cd1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_frames=20, backbone=\"efficientnet_b4\", dropout_rate=0.6):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # CNN Backbone for feature extraction\n",
    "        if backbone == \"efficientnet_b4\":\n",
    "            weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.efficientnet_b4(weights=weights)\n",
    "            self.backbone.classifier = nn.Identity()  # Remove final classifier\n",
    "            feature_dim = 1792\n",
    "        elif backbone == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            feature_dim = 2048\n",
    "\n",
    "        # Temporal processing layers (LSTM)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=512,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism to focus on important frames\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # 512 * 2 for bidirectional LSTM\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = (lstm_out * attention_weights).sum(dim=1)\n",
    "\n",
    "        output = self.classifier(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb74e28-bf2b-449c-a6a7-322c92c62f02",
   "metadata": {},
   "source": [
    "# Load Model and set up for inference\n",
    "This section handles loading the pre-trained model and preparing all necessary components for the inference process, including the face detector (MTCNN) and image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f602a94-ec9c-4f26-948f-ea5505c20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dependencies():\n",
    "    \"\"\"\n",
    "    Initializes the model, loads weights, and sets up dependencies.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    MODEL_PATH = (\n",
    "        \"models/best_deepfake_detector_model.pth\"  # IMPORTANT: Update this path\n",
    "    )\n",
    "    NUM_FRAMES = 15  # Must match the model's training configuration\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # Set device (GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = DeepfakeDetector(num_frames=NUM_FRAMES, backbone=\"efficientnet_b4\").to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Load the trained weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        logging.info(f\"Successfully loaded model weights from {MODEL_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\n",
    "            f\"Model file not found at {MODEL_PATH}. Please update the MODEL_PATH variable.\"\n",
    "        )\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize MTCNN for face detection\n",
    "    mtcnn = MTCNN(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        margin=20,\n",
    "        device=device,\n",
    "        min_face_size=20,\n",
    "        thresholds=[0.6, 0.7, 0.7],\n",
    "    )\n",
    "    logging.info(\"MTCNN face detector initialized.\")\n",
    "\n",
    "    # Define validation transforms (must match those used during training)\n",
    "    val_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model, mtcnn, val_transforms, device, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796cd75-d1c9-4cb6-ac35-761ef0a19aca",
   "metadata": {},
   "source": [
    "# Video inference and bounding box visualization\n",
    "This function performs the core task: processing a video frame by frame, detecting faces, running the deepfake prediction, and drawing bounding boxes with the results on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec79c3ab-46da-4065-b7ab-10311e0332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_deepfake_in_video(\n",
    "    video_path, output_path, model, mtcnn, transforms, device, image_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video to detect deepfakes, draws bounding boxes, and saves the output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        logging.error(f\"Video not found at {video_path}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Starting deepfake detection on {video_path}\")\n",
    "\n",
    "    # Create directories for frame outputs\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    frames_output_dir = os.path.join(\n",
    "        output_dir, \"processed_frames\", datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    )\n",
    "\n",
    "    os.makedirs(frames_output_dir, exist_ok=True)\n",
    "\n",
    "    # List to hold prediction data for the CSV\n",
    "    prediction_data = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        f\"{output_path}/output_video.mp4\", fourcc, fps, (width, height)\n",
    "    )\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    face_buffer = []\n",
    "    frame_buffer = []\n",
    "    BUFFER_SIZE = model.num_frames  # Should be 15\n",
    "\n",
    "    all_probs = []  # A list to store all frame probabilities\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "            boxes, probs = mtcnn.detect(pil_image)\n",
    "\n",
    "            # Check if any faces were detected\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                # Find the index of the face with the highest detection probability\n",
    "                best_face_index = np.argmax(probs)\n",
    "                best_face_prob = probs[best_face_index]\n",
    "\n",
    "                # Only proceed if the best detection is high-confidence\n",
    "                if best_face_prob > 0.95:\n",
    "                    box = boxes[best_face_index]\n",
    "\n",
    "                    x1, y1, x2, y2 = [int(b) for b in box]\n",
    "                    face = pil_image.crop((x1, y1, x2, y2))\n",
    "                    face_resized = face.resize((image_size, image_size))\n",
    "                    facetensor = transforms(face_resized)\n",
    "\n",
    "                    face_buffer.append(facetensor)\n",
    "                    frame_buffer.append(\n",
    "                        {\n",
    "                            \"frame\": frame.copy(),\n",
    "                            \"box\": box,\n",
    "                            \"frame_count\": frame_count,\n",
    "                            \"face_index\": best_face_index,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if len(face_buffer) == BUFFER_SIZE:\n",
    "                        input_tensor = torch.stack(face_buffer).unsqueeze(0).to(device)\n",
    "\n",
    "                        prediction = model(input_tensor)\n",
    "                        prediction_prob = torch.sigmoid(prediction).item()\n",
    "                        all_probs.append(prediction_prob)\n",
    "\n",
    "                        # Always label with the FAKE probability\n",
    "                        display_text = f\"FAKE: {prediction_prob:.2f}\"\n",
    "                        # Change color based on the threshold\n",
    "                        color = (\n",
    "                            (0, 0, 255) if prediction_prob > 0.5 else (0, 255, 0)\n",
    "                        )  # Red for > 0.5, Green for <= 0.5\n",
    "                        label_text = (\n",
    "                            \"FAKE\" if prediction_prob > 0.5 else \"REAL\"\n",
    "                        )  # Keep this for the CSV log\n",
    "\n",
    "                        for item in frame_buffer:\n",
    "                            f, b, fc, face_idx = (\n",
    "                                item[\"frame\"],\n",
    "                                item[\"box\"],\n",
    "                                item[\"frame_count\"],\n",
    "                                item[\"face_index\"],\n",
    "                            )\n",
    "                            x1_b, y1_b, x2_b, y2_b = [int(coord) for coord in b]\n",
    "\n",
    "                            # Draw bounding box and label\n",
    "                            cv2.rectangle(f, (x1_b, y1_b), (x2_b, y2_b), color, 2)\n",
    "                            cv2.putText(\n",
    "                                f,\n",
    "                                display_text,\n",
    "                                (x1_b, y1_b - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                0.7,\n",
    "                                color,\n",
    "                                2,\n",
    "                            )\n",
    "\n",
    "                            # Write the ANNOTATED frame to the video\n",
    "                            out.write(f)\n",
    "\n",
    "                            # Save the annotated frame as an image\n",
    "                            frame_filename = f\"frame{fc}_face{face_idx}.jpg\" # Use face_idx here\n",
    "                            frame_save_path = os.path.join(\n",
    "                                frames_output_dir, frame_filename\n",
    "                            )\n",
    "                            cv2.imwrite(frame_save_path, f)\n",
    "\n",
    "                            # Append prediction data for this frame\n",
    "                            prediction_data.append(\n",
    "                                {\n",
    "                                    \"frame_number\": fc,\n",
    "                                    \"face_index\": face_idx,\n",
    "                                    \"bounding_box\": f\"({int(b[0])},{int(b[1])},{int(b[2])},{int(b[3])})\",\n",
    "                                    \"prediction_prob\": prediction_prob,\n",
    "                                    \"label\": label_text,\n",
    "                                    \"face_image_path\": frame_save_path,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                        face_buffer.clear()\n",
    "                        frame_buffer.clear()\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                logging.info(f\"Processed {frame_count} frames...\")\n",
    "        if face_buffer:\n",
    "            # Pad the buffer if it's not full\n",
    "            padding_needed = BUFFER_SIZE - len(face_buffer)\n",
    "            if padding_needed > 0:\n",
    "                last_face_tensor = face_buffer[-1]\n",
    "                face_buffer.extend([last_face_tensor] * padding_needed)\n",
    "\n",
    "            input_tensor = torch.stack(face_buffer).unsqueeze(0).to(device)\n",
    "\n",
    "            # Perform prediction\n",
    "            prediction = model(input_tensor)\n",
    "            prediction_prob = torch.sigmoid(prediction).item()\n",
    "            all_probs.append(prediction_prob)\n",
    "\n",
    "            # Determine label and color\n",
    "            label = (\n",
    "                f\"FAKE {prediction_prob:.2f}\"\n",
    "                if prediction_prob > 0.5\n",
    "                else f\"REAL {1 - prediction_prob:.2f}\"\n",
    "            )\n",
    "            color = (0, 0, 255) if prediction_prob > 0.5 else (0, 255, 0)\n",
    "\n",
    "            # Draw on remaining buffered frames\n",
    "            for item in frame_buffer:\n",
    "                f, b = item[\"frame\"], item[\"box\"]\n",
    "                x1_b, y1_b, x2_b, y2_b = [int(coord) for coord in b]\n",
    "                cv2.rectangle(f, (x1_b, y1_b), (x2_b, y2_b), color, 2)\n",
    "                cv2.putText(\n",
    "                    f, label, (x1_b, y1_b - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2\n",
    "                )\n",
    "                out.write(f)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    if prediction_data:\n",
    "        df = pd.DataFrame(prediction_data)\n",
    "        csv_output_path = os.path.join(output_dir, \"prediction_results.csv\")\n",
    "        df.to_csv(csv_output_path, index=False, float_format=\"%.2f\")\n",
    "        logging.info(f\"Prediction results saved to {csv_output_path}\")\n",
    "\n",
    "    if all_probs:\n",
    "        avg_prob = np.mean(all_probs)\n",
    "        confidence = avg_prob\n",
    "        if avg_prob > 0.5:\n",
    "            final_verdict = \"FAKE\"\n",
    "        else:\n",
    "            final_verdict = \"REAL\"\n",
    "        logging.info(f\"--- Video Analysis Complete ---\")\n",
    "        logging.info(f\"Final Verdict: The video is likely {final_verdict}\")\n",
    "        logging.info(f\"Average FAKE Confidence: {confidence:.2%}\")\n",
    "    else:\n",
    "        logging.info(\"No faces were confidently detected in the video.\")\n",
    "\n",
    "    logging.info(f\"Detection complete. Output video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d7768-c2eb-47e8-9d48-727ed7c26219",
   "metadata": {},
   "source": [
    "# Main execution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59f386e-92bd-497d-8376-f958546cfc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 04:32:43,874 - INFO - Using device: cuda\n",
      "2025-10-19 04:32:44,548 - INFO - Successfully loaded model weights from models/best_deepfake_detector_model.pth\n",
      "2025-10-19 04:32:44,559 - INFO - MTCNN face detector initialized.\n",
      "2025-10-19 04:32:44,560 - INFO - Starting deepfake detection on ./id59_id61_0006.mp4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     os.makedirs(output_dir)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run the deepfake detection process\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mdetect_deepfake_in_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINPUT_VIDEO_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmtcnn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmtcnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mdetect_deepfake_in_video\u001b[39m\u001b[34m(video_path, output_path, model, mtcnn, transforms, device, image_size)\u001b[39m\n\u001b[32m    104\u001b[39m out.write(f)\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Save the annotated frame as an image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m frame_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mframe\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_face\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mi\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m frame_save_path = os.path.join(\n\u001b[32m    109\u001b[39m     frames_output_dir, frame_filename\n\u001b[32m    110\u001b[39m )\n\u001b[32m    111\u001b[39m cv2.imwrite(frame_save_path, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- User Configuration ---\n",
    "\n",
    "    INPUT_VIDEO_PATH = \"./id59_id61_0006.mp4\"\n",
    "    OUTPUT_PATH = \".\"\n",
    "\n",
    "    # Load model and dependencies\n",
    "    model, mtcnn, val_transforms, device, image_size = load_model_and_dependencies()\n",
    "\n",
    "    if model:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(OUTPUT_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Run the deepfake detection process\n",
    "        detect_deepfake_in_video(\n",
    "            video_path=INPUT_VIDEO_PATH,\n",
    "            output_path=OUTPUT_PATH,\n",
    "            model=model,\n",
    "            mtcnn=mtcnn,\n",
    "            transforms=val_transforms,\n",
    "            device=device,\n",
    "            image_size=image_size,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
