{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a978853e-75b3-412f-a4a9-b8b1e19d7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Setup logging to display progress and informational messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792d72f-a5b6-4fda-9a7d-fb345d996a20",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "To load the saved model weights, we must first define the model's architecture. This class must be identical to the `DeepfakeDetector` class used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfa5319-34c6-4789-b816-8e5615cd1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_frames=20, backbone=\"efficientnet_b4\", dropout_rate=0.6):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # CNN Backbone for feature extraction\n",
    "        if backbone == \"efficientnet_b4\":\n",
    "            weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.efficientnet_b4(weights=weights)\n",
    "            self.backbone.classifier = nn.Identity()  # Remove final classifier\n",
    "            feature_dim = 1792\n",
    "        elif backbone == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            feature_dim = 2048\n",
    "\n",
    "        # Temporal processing layers (LSTM)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=512,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism to focus on important frames\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # 512 * 2 for bidirectional LSTM\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = (lstm_out * attention_weights).sum(dim=1)\n",
    "\n",
    "        output = self.classifier(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb74e28-bf2b-449c-a6a7-322c92c62f02",
   "metadata": {},
   "source": [
    "# Load Model and set up for inference\n",
    "This section handles loading the pre-trained model and preparing all necessary components for the inference process, including the face detector (MTCNN) and image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f602a94-ec9c-4f26-948f-ea5505c20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dependencies():\n",
    "    \"\"\"\n",
    "    Initializes the model, loads weights, and sets up dependencies.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    MODEL_PATH = (\n",
    "        \"models/best_deepfake_detector_model.pth\"  # IMPORTANT: Update this path\n",
    "    )\n",
    "    NUM_FRAMES = 15  # Must match the model's training configuration\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # Set device (GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = DeepfakeDetector(num_frames=NUM_FRAMES, backbone=\"efficientnet_b4\").to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Load the trained weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        logging.info(f\"Successfully loaded model weights from {MODEL_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\n",
    "            f\"Model file not found at {MODEL_PATH}. Please update the MODEL_PATH variable.\"\n",
    "        )\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize MTCNN for face detection\n",
    "    mtcnn = MTCNN(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        margin=20,\n",
    "        post_process=False,\n",
    "        device=device,\n",
    "        select_largest=True,  # Handle multiple faces per frame,\n",
    "        selection_method=\"probability\"  # Select the face with the highest confidence\n",
    "    )\n",
    "    logging.info(\"MTCNN face detector initialized.\")\n",
    "\n",
    "    # Define validation transforms (must match those used during training)\n",
    "    val_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model, mtcnn, val_transforms, device, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796cd75-d1c9-4cb6-ac35-761ef0a19aca",
   "metadata": {},
   "source": [
    "# Video inference and bounding box visualization\n",
    "This function performs the core task: processing a video frame by frame, detecting faces, running the deepfake prediction, and drawing bounding boxes with the results on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec79c3ab-46da-4065-b7ab-10311e0332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_deepfake_in_video(\n",
    "    video_path, output_path, model, mtcnn, transforms, device, image_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video to detect deepfakes, draws bounding boxes, and saves the output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        logging.error(f\"Video not found at {video_path}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Starting deepfake detection on {video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    with torch.no_grad():\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert from BGR (OpenCV) to RGB for PIL and model\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Detect faces and their bounding boxes\n",
    "            boxes, _ = mtcnn.detect(pil_image)\n",
    "\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "                    # Extract face using bounding box\n",
    "                    face = pil_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "                    # Preprocess the face\n",
    "                    face_resized = face.resize((image_size, image_size))\n",
    "                    face_tensor = transforms(face_resized)\n",
    "\n",
    "                    # The model expects a sequence of frames. We replicate the single face\n",
    "                    # tensor to match the required input shape [1, num_frames, C, H, W].\n",
    "                    input_tensor = (\n",
    "                        face_tensor.unsqueeze(0)\n",
    "                        .repeat(model.num_frames, 1, 1, 1)\n",
    "                        .unsqueeze(0)\n",
    "                        .to(device)\n",
    "                    )\n",
    "\n",
    "                    # Perform prediction\n",
    "                    prediction = model(input_tensor)\n",
    "                    prob = torch.sigmoid(prediction).item()\n",
    "\n",
    "                    # Determine label and color for the bounding box\n",
    "                    if prob > 0.5:\n",
    "                        label = f\"FAKE: {prob:.2%}\"\n",
    "                        color = (0, 0, 255)  # Red for Fake\n",
    "                    else:\n",
    "                        label = f\"REAL: {1-prob:.2%}\"\n",
    "                        color = (0, 255, 0)  # Green for Real\n",
    "\n",
    "                    # Draw bounding box and label on the original frame\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        label,\n",
    "                        (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7,\n",
    "                        color,\n",
    "                        2,\n",
    "                    )\n",
    "\n",
    "            out.write(frame)\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                logging.info(f\"Processed {frame_count} frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    logging.info(f\"Detection complete. Output video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d7768-c2eb-47e8-9d48-727ed7c26219",
   "metadata": {},
   "source": [
    "# Main execution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59f386e-92bd-497d-8376-f958546cfc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 12:18:38,807 - INFO - Using device: cuda\n",
      "2025-10-09 12:18:39,525 - INFO - Successfully loaded model weights from models/best_deepfake_detector_model.pth\n",
      "2025-10-09 12:18:39,537 - INFO - MTCNN face detector initialized.\n",
      "2025-10-09 12:18:39,537 - INFO - Starting deepfake detection on ./id59_id61_0006.mp4\n",
      "2025-10-09 12:18:43,614 - INFO - Processed 100 frames...\n",
      "2025-10-09 12:18:47,127 - INFO - Processed 200 frames...\n",
      "2025-10-09 12:18:51,206 - INFO - Processed 300 frames...\n",
      "2025-10-09 12:18:55,595 - INFO - Processed 400 frames...\n",
      "2025-10-09 12:18:58,516 - INFO - Detection complete. Output video saved to ./output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- User Configuration ---\n",
    "\n",
    "    INPUT_VIDEO_PATH = \"./id59_id61_0006.mp4\"\n",
    "    OUTPUT_VIDEO_PATH = \"./output_video.mp4\"\n",
    "\n",
    "    # Load model and dependencies\n",
    "    model, mtcnn, val_transforms, device, image_size = load_model_and_dependencies()\n",
    "\n",
    "    if model:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(OUTPUT_VIDEO_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Run the deepfake detection process\n",
    "        detect_deepfake_in_video(\n",
    "            video_path=INPUT_VIDEO_PATH,\n",
    "            output_path=OUTPUT_VIDEO_PATH,\n",
    "            model=model,\n",
    "            mtcnn=mtcnn,\n",
    "            transforms=val_transforms,\n",
    "            device=device,\n",
    "            image_size=image_size,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
