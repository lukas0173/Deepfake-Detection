{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "from deepfake_dataset import DeepfakeDataset, collate_fn\n",
    "from worker import worker_init_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_VIDEO_PATH = \"./dataset\"\n",
    "METADATA_PATH = \"./dataset/celeb_df_metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Logging Setup: This section imports necessary libraries (e.g., cv2 for video handling, torch for tensors, MTCNN for face detection) and configures logging. Logging is crucial for robustness, as the PDF highlights potential issues like corrupted videos or face detection failures; it allows tracking warnings/errors without crashing the pipeline.\n",
    "\n",
    "Class Initialization (__init__): Initializes the dataset with metadata, transformations, frame count, and image size. It sets up MTCNN for face detection on GPU if available, ensuring efficiency for large datasets like Celeb-DF-v2 with varying video qualities.\n",
    "\n",
    "Length Method (__len__): Returns the number of videos in the metadata, enabling PyTorch's DataLoader to iterate correctly. This is standard but essential for handling the imbalanced dataset size (890 real vs. 5639 fake).\n",
    "\n",
    "Face Extraction Method (extract_faces_from_video): Handles video reading with error checks (e.g., file existence, opening failures) to prevent crashes on problematic files. It uses adaptive sampling (random for short videos, uniform for longer ones) to capture temporal information without bias, extracts faces via MTCNN, and logs failures for debugging.\n",
    "\n",
    "Item Retrieval Method (__getitem__): Fetches a video's data by index, assigns binary labels (0 for real, 1 for fake), extracts faces, and handles shortages with noise-added repetitions to avoid overfitting on duplicates. It applies transformations and stacks frames into a tensor, preparing consistent inputs despite dataset variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging for robustness\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# Reconfigure the root logger to ensure output appears in the notebook\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []  # Clear any existing handlers\n",
    "handler = logging.StreamHandler(\n",
    "    sys.stdout\n",
    ")  # Create a new handler to stream to the console\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation and Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Transforms (train_transforms): Composes a sequence of augmentations starting with PIL conversion for compatibility, resizing to 224x224 (standard for backbones like EfficientNet), and random flips/rotations to simulate pose variations. Color jitter and affine shear address lighting/quality inconsistencies, enhancing robustness without masking deepfake artifacts; normalization uses ImageNet stats for transfer learning.\n",
    "\n",
    "Validation Transforms (val_transforms): A minimal composition for evaluation, including PIL conversion, resizing, tensor conversion, and normalization. This ensures consistent inputs without random augmentations, allowing fair assessment of model generalization on the imbalanced validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define enhanced data augmentation for training robustness, considering dataset variability\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, shear=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Initialization (__init__): Sets up the model with frame count (20 for better temporal coverage), selects a pretrained CNN backbone (e.g., EfficientNet-B4 for efficiency on facial details), and defines bidirectional LSTMs for temporal analysis, attention layers for focusing on key frames, and a classifier with batch norm/dropout for stability on imbalanced data.\n",
    "\n",
    "Forward Pass (forward): Reshapes input for batch CNN processing, extracts features with mixed precision for speed, reshapes for LSTM, applies bidirectional temporal modeling to detect inconsistencies, weights frames via attention (useful for variable-length videos), and classifies via dense layers, outputting a sigmoid probability for real/fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_frames=20, backbone=\"efficientnet_b4\", dropout_rate=0.6):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # CNN Backbone for feature extraction\n",
    "        if backbone == \"efficientnet_b4\":\n",
    "            weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "            # self.backbone = models.efficientnet_b4(pretrained=True)\n",
    "            self.backbone = models.efficientnet_b4(weights=weights)\n",
    "            self.backbone.classifier = nn.Identity()  # Remove final classifier\n",
    "            feature_dim = 1792\n",
    "        elif backbone == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            # self.backbone = models.resnet50(pretrained=True)\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            feature_dim = 2048\n",
    "\n",
    "        # Temporal processing layers with bidirectional LSTM for better sequence modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=512,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism for frame importance weighting\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1024, 256), nn.ReLU(), nn.Linear(256, 1), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        # Final classification layers with additional regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # Adjusted for bidirectional\n",
    "            nn.BatchNorm1d(256),  # Added for stability\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "        # Use torch.amp.autocast for mixed precision\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            features = self.backbone(x)\n",
    "\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "\n",
    "        # The correct approach is to multiply lstm_out by the attention weights\n",
    "        # and then sum across the time dimension.\n",
    "        # weighted_features = (lstm_out * attention_weights).sum(dim=1)\n",
    "        # weighted_features = lstm_out * attention_weights + lstm_out  # Helps with gradient flow.\n",
    "        # weighted_features = weighted_features.mean(dim=1)  # Reduce over sequence dimension (average pooling)\n",
    "\n",
    "        # output = self.classifier(weighted_features)\n",
    "        context_vector = (lstm_out * attention_weights).sum(dim=1)\n",
    "        output = self.classifier(context_vector)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Setup Function (setup_training): Loads metadata, performs stratified split to preserve imbalance ratios, creates datasets, computes sample weights for oversampling (addressing PDF's 1:6 imbalance), and sets up DataLoaders with a sampler for balanced batching.\n",
    "\n",
    "Training Function (train_model): Configures device/loss/optimizer with weighted BCE for imbalance, initializes mixed-precision scaler, and runs epochs with training loops (forward pass, backprop with clipping for stability), validation (no-grad inference), metrics calculation, scheduling, and early stopping to save the best model and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.amp\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss()(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        return self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "\n",
    "\n",
    "\n",
    "def setup_training():\n",
    "    # Load metadata from dataset analysis\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    # metadata['filepath'] = metadata['filepath'].apply(lambda x: os.path.join(BASE_VIDEO_PATH, x))\n",
    "    metadata[\"filepath\"] = metadata[\"filepath\"].apply(\n",
    "        # lambda x: os.path.join(BASE_VIDEO_PATH, x)\n",
    "        lambda x: os.path.join(BASE_VIDEO_PATH, x)\n",
    "    )\n",
    "\n",
    "    # Train/validation split with stratification\n",
    "    train_df, val_df = train_test_split(\n",
    "        metadata, test_size=0.2, stratify=metadata[\"label\"], random_state=42\n",
    "    )\n",
    "\n",
    "    # Define a cache directory in a writable location\n",
    "    face_cache_directory = \"/kaggle/working/face_cache\"\n",
    "    # Create datasets\n",
    "    train_dataset = DeepfakeDataset(train_df, transform=train_transforms)\n",
    "    val_dataset = DeepfakeDataset(val_df, transform=val_transforms)\n",
    "\n",
    "    # Compute class weights for oversampling to handle imbalance (real: ~890, fake: ~5639 total)\n",
    "    class_counts = metadata[\"label\"].value_counts()\n",
    "    # class_weights = {0: 1.0 / class_counts['real'], 1: 1.0 / class_counts['fake']}\n",
    "    class_weights = {\n",
    "        0: class_counts[\"fake\"] / class_counts[\"real\"],\n",
    "        1: 1.0,\n",
    "    }  # This oversamples real videos more heavily.\n",
    "\n",
    "    # sample_weights = [class_weights[0] if label == 'real' else class_weights[1] for label in train_df['label']]\n",
    "    # sample_weights = [class_weights[label] for label in train_df['label']]\n",
    "    sample_weights = [\n",
    "        class_weights[1 if label == \"fake\" else 0] for label in train_df[\"label\"]\n",
    "    ]\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        sample_weights, len(sample_weights), replacement=True\n",
    "    )\n",
    "\n",
    "    # Calculate pos_weight from the dataframe\n",
    "    # This is safe because it's before any data loading failures\n",
    "    real_count = (train_df[\"label\"] == \"real\").sum()\n",
    "    fake_count = (train_df[\"label\"] == \"fake\").sum()\n",
    "    pos_weight = torch.tensor([fake_count / real_count])\n",
    "\n",
    "    # Create data loaders with oversampling\n",
    "    # Apply the collate_fn to both training and validation loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=6,\n",
    "        shuffle=False,  # Shuffle is handled by WeightedRandomSampler\n",
    "        sampler=sampler,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=6,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, pos_weight\n",
    "\n",
    "\n",
    "# Training function with early stopping and gradient clipping\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    pos_weight,\n",
    "    num_epochs=30,\n",
    "    learning_rate=1e-5,\n",
    "    patience=10,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "    # Use BCEWithLogitsLoss for numerical stability\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=3, factor=0.5\n",
    "    )\n",
    "\n",
    "    criterion = FocalLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-4\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0.0, 0\n",
    "\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            if data.nelement() == 0:  # Skip empty batches\n",
    "                continue\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            targets = targets.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use torch.amp.autocast\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "\n",
    "            scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (predictions == targets).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(\n",
    "                    f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0.0, 0\n",
    "        all_predictions, all_targets = [], []\n",
    "\n",
    "        with torch.no_grad(), autocast():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                targets = targets.unsqueeze(1)\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = (probabilities > 0.5).float()\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (predictions == targets).sum().item()\n",
    "\n",
    "                all_predictions.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        auc_score = roc_auc_score(all_targets, all_predictions)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, AUC: {auc_score:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_deepfake_detector_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Function (evaluate_model): Sets model to eval mode, collects predictions/probabilities with no-grad and mixed precision, computes metrics like accuracy/AUC/PR curves, and calculates EER (key for imbalanced security tasks per PDF). It generates and saves plots for visual analysis.\n",
    "\n",
    "Main Execution Block (if __name__ == \"__main__\"): Orchestrates the pipeline by setting up loaders, initializing the model, training, evaluating, and saving the final model, with logging for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:39:54,754 - INFO - Initializing deepfake detection model...\n",
      "2025-10-07 04:39:55,484 - INFO - Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7887/2150798169.py:120: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:40:26,582 - INFO - Epoch 1/20, Batch 0/871, Loss: 0.0567\n",
      "2025-10-07 04:40:44,857 - INFO - Epoch 1/20, Batch 10/871, Loss: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:40:52,302] [INFO] [Worker ID: 8331]: Processing video 100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:41:13,594 - INFO - Epoch 1/20, Batch 20/871, Loss: 0.0422\n",
      "2025-10-07 04:41:38,061 - INFO - Epoch 1/20, Batch 30/871, Loss: 0.0502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:41:39,007] [INFO] [Worker ID: 8259]: Processing video 200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:42:08,330 - INFO - Epoch 1/20, Batch 40/871, Loss: 0.0482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:42:24,590] [INFO] [Worker ID: 8126]: Processing video 300/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:42:33,698 - INFO - Epoch 1/20, Batch 50/871, Loss: 0.0407\n",
      "2025-10-07 04:43:07,246 - INFO - Epoch 1/20, Batch 60/871, Loss: 0.0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:43:12,372] [INFO] [Worker ID: 8259]: Processing video 400/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:43:36,141 - INFO - Epoch 1/20, Batch 70/871, Loss: 0.0438\n",
      "2025-10-07 04:44:00,820 - INFO - Epoch 1/20, Batch 80/871, Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:44:02,551] [INFO] [Worker ID: 8192]: Processing video 500/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:44:34,949 - INFO - Epoch 1/20, Batch 90/871, Loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:44:51,858] [INFO] [Worker ID: 8126]: Processing video 600/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:44:59,790 - INFO - Epoch 1/20, Batch 100/871, Loss: 0.0496\n",
      "2025-10-07 04:45:31,049 - INFO - Epoch 1/20, Batch 110/871, Loss: 0.0459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:45:41,616] [INFO] [Worker ID: 8192]: Processing video 700/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:45:58,424 - INFO - Epoch 1/20, Batch 120/871, Loss: 0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:46:31,144] [INFO] [Worker ID: 8331]: Processing video 800/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:46:31,865 - INFO - Epoch 1/20, Batch 130/871, Loss: 0.0449\n",
      "2025-10-07 04:47:03,962 - INFO - Epoch 1/20, Batch 140/871, Loss: 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:47:21,664] [INFO] [Worker ID: 8126]: Processing video 900/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:47:30,253 - INFO - Epoch 1/20, Batch 150/871, Loss: 0.0394\n",
      "2025-10-07 04:47:57,432 - INFO - Epoch 1/20, Batch 160/871, Loss: 0.0399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:48:09,463] [INFO] [Worker ID: 8259]: Processing video 1000/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:48:31,863 - INFO - Epoch 1/20, Batch 170/871, Loss: 0.0395\n",
      "2025-10-07 04:48:58,816 - INFO - Epoch 1/20, Batch 180/871, Loss: 0.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:48:59,891] [INFO] [Worker ID: 8331]: Processing video 1100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:49:30,509 - INFO - Epoch 1/20, Batch 190/871, Loss: 0.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:49:46,805] [INFO] [Worker ID: 8331]: Processing video 1200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:49:53,743 - INFO - Epoch 1/20, Batch 200/871, Loss: 0.0523\n",
      "2025-10-07 04:50:30,733 - INFO - Epoch 1/20, Batch 210/871, Loss: 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:50:37,007] [INFO] [Worker ID: 8126]: Processing video 1300/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:50:55,293 - INFO - Epoch 1/20, Batch 220/871, Loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:51:26,782] [INFO] [Worker ID: 8192]: Processing video 1400/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:51:27,002 - INFO - Epoch 1/20, Batch 230/871, Loss: 0.0434\n",
      "2025-10-07 04:51:54,183 - INFO - Epoch 1/20, Batch 240/871, Loss: 0.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:52:15,308] [INFO] [Worker ID: 8126]: Processing video 1500/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:52:25,308 - INFO - Epoch 1/20, Batch 250/871, Loss: 0.0381\n",
      "2025-10-07 04:52:55,704 - INFO - Epoch 1/20, Batch 260/871, Loss: 0.0554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:53:06,507] [INFO] [Worker ID: 8331]: Processing video 1600/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:53:24,234 - INFO - Epoch 1/20, Batch 270/871, Loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:53:55,084] [INFO] [Worker ID: 8126]: Processing video 1700/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:53:56,870 - INFO - Epoch 1/20, Batch 280/871, Loss: 0.0450\n",
      "2025-10-07 04:54:33,697 - INFO - Epoch 1/20, Batch 290/871, Loss: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:54:50,247] [INFO] [Worker ID: 8259]: Processing video 1800/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:55:04,695 - INFO - Epoch 1/20, Batch 300/871, Loss: 0.0435\n",
      "2025-10-07 04:55:28,525 - INFO - Epoch 1/20, Batch 310/871, Loss: 0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:55:37,756] [INFO] [Worker ID: 8259]: Processing video 1900/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:56:00,386 - INFO - Epoch 1/20, Batch 320/871, Loss: 0.0528\n",
      "2025-10-07 04:56:23,154 - INFO - Epoch 1/20, Batch 330/871, Loss: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:56:25,976] [INFO] [Worker ID: 8259]: Processing video 2000/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:57:02,846 - INFO - Epoch 1/20, Batch 340/871, Loss: 0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:57:17,902] [INFO] [Worker ID: 8192]: Processing video 2100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:57:28,264 - INFO - Epoch 1/20, Batch 350/871, Loss: 0.0560\n",
      "2025-10-07 04:58:02,653 - INFO - Epoch 1/20, Batch 360/871, Loss: 0.0464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:58:06,604] [INFO] [Worker ID: 8192]: Processing video 2200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:58:28,869 - INFO - Epoch 1/20, Batch 370/871, Loss: 0.0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:58:54,967] [INFO] [Worker ID: 8192]: Processing video 2300/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:58:59,848 - INFO - Epoch 1/20, Batch 380/871, Loss: 0.0505\n",
      "2025-10-07 04:59:27,309 - INFO - Epoch 1/20, Batch 390/871, Loss: 0.0560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 04:59:44,166] [INFO] [Worker ID: 8192]: Processing video 2400/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 04:59:56,665 - INFO - Epoch 1/20, Batch 400/871, Loss: 0.0434\n",
      "2025-10-07 05:00:30,135 - INFO - Epoch 1/20, Batch 410/871, Loss: 0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:00:35,300] [INFO] [Worker ID: 8192]: Processing video 2500/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:01:03,300 - INFO - Epoch 1/20, Batch 420/871, Loss: 0.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:01:24,435] [INFO] [Worker ID: 8331]: Processing video 2600/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:01:29,338 - INFO - Epoch 1/20, Batch 430/871, Loss: 0.0396\n",
      "2025-10-07 05:02:00,857 - INFO - Epoch 1/20, Batch 440/871, Loss: 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:02:10,337] [INFO] [Worker ID: 8192]: Processing video 2700/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:02:23,967 - INFO - Epoch 1/20, Batch 450/871, Loss: 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:03:02,803] [INFO] [Worker ID: 8259]: Processing video 2800/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:03:04,281 - INFO - Epoch 1/20, Batch 460/871, Loss: 0.0485\n",
      "2025-10-07 05:03:28,861 - INFO - Epoch 1/20, Batch 470/871, Loss: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:03:52,644] [INFO] [Worker ID: 8192]: Processing video 2900/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:03:58,100 - INFO - Epoch 1/20, Batch 480/871, Loss: 0.0378\n",
      "2025-10-07 05:04:22,814 - INFO - Epoch 1/20, Batch 490/871, Loss: 0.0361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:04:41,068] [INFO] [Worker ID: 8192]: Processing video 3000/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:04:54,243 - INFO - Epoch 1/20, Batch 500/871, Loss: 0.0480\n",
      "2025-10-07 05:05:23,742 - INFO - Epoch 1/20, Batch 510/871, Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:05:27,893] [INFO] [Worker ID: 8259]: Processing video 3100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:05:50,701 - INFO - Epoch 1/20, Batch 520/871, Loss: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:06:20,534] [INFO] [Worker ID: 8192]: Processing video 3200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:06:30,299 - INFO - Epoch 1/20, Batch 530/871, Loss: 0.0390\n",
      "2025-10-07 05:06:50,975 - INFO - Epoch 1/20, Batch 540/871, Loss: 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:07:08,682] [INFO] [Worker ID: 8331]: Processing video 3300/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:07:25,726 - INFO - Epoch 1/20, Batch 550/871, Loss: 0.0514\n",
      "2025-10-07 05:07:48,182 - INFO - Epoch 1/20, Batch 560/871, Loss: 0.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:07:59,298] [INFO] [Worker ID: 8331]: Processing video 3400/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:08:19,184 - INFO - Epoch 1/20, Batch 570/871, Loss: 0.0432\n",
      "2025-10-07 05:08:45,796 - INFO - Epoch 1/20, Batch 580/871, Loss: 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:08:46,485] [INFO] [Worker ID: 8126]: Processing video 3500/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:09:12,545 - INFO - Epoch 1/20, Batch 590/871, Loss: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:09:36,681] [INFO] [Worker ID: 8259]: Processing video 3600/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:09:51,348 - INFO - Epoch 1/20, Batch 600/871, Loss: 0.0353\n",
      "2025-10-07 05:10:18,922 - INFO - Epoch 1/20, Batch 610/871, Loss: 0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:10:25,939] [INFO] [Worker ID: 8331]: Processing video 3700/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:10:54,655 - INFO - Epoch 1/20, Batch 620/871, Loss: 0.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:11:17,550] [INFO] [Worker ID: 8126]: Processing video 3800/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:11:20,658 - INFO - Epoch 1/20, Batch 630/871, Loss: 0.0534\n",
      "2025-10-07 05:11:56,430 - INFO - Epoch 1/20, Batch 640/871, Loss: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:12:06,593] [INFO] [Worker ID: 8331]: Processing video 3900/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:12:21,185 - INFO - Epoch 1/20, Batch 650/871, Loss: 0.0572\n",
      "2025-10-07 05:12:49,773 - INFO - Epoch 1/20, Batch 660/871, Loss: 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:12:57,399] [INFO] [Worker ID: 8192]: Processing video 4000/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:13:17,662 - INFO - Epoch 1/20, Batch 670/871, Loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:13:45,144] [INFO] [Worker ID: 8192]: Processing video 4100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:13:54,734 - INFO - Epoch 1/20, Batch 680/871, Loss: 0.0452\n",
      "2025-10-07 05:14:19,771 - INFO - Epoch 1/20, Batch 690/871, Loss: 0.0286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:14:36,469] [INFO] [Worker ID: 8126]: Processing video 4200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:14:57,493 - INFO - Epoch 1/20, Batch 700/871, Loss: 0.0374\n",
      "2025-10-07 05:15:22,764 - INFO - Epoch 1/20, Batch 710/871, Loss: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:15:26,600] [INFO] [Worker ID: 8259]: Processing video 4300/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:16:01,319 - INFO - Epoch 1/20, Batch 720/871, Loss: 0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:16:17,797] [INFO] [Worker ID: 8126]: Processing video 4400/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:16:21,309 - INFO - Epoch 1/20, Batch 730/871, Loss: 0.0462\n",
      "2025-10-07 05:16:56,855 - INFO - Epoch 1/20, Batch 740/871, Loss: 0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:17:08,457] [INFO] [Worker ID: 8331]: Processing video 4500/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:17:21,756 - INFO - Epoch 1/20, Batch 750/871, Loss: 0.0534\n",
      "2025-10-07 05:17:54,591 - INFO - Epoch 1/20, Batch 760/871, Loss: 0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:18:00,056] [INFO] [Worker ID: 8192]: Processing video 4600/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:18:20,075 - INFO - Epoch 1/20, Batch 770/871, Loss: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:18:48,152] [INFO] [Worker ID: 8259]: Processing video 4700/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:18:52,503 - INFO - Epoch 1/20, Batch 780/871, Loss: 0.0461\n",
      "2025-10-07 05:19:17,556 - INFO - Epoch 1/20, Batch 790/871, Loss: 0.0494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:19:38,924] [INFO] [Worker ID: 8192]: Processing video 4800/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:19:46,825 - INFO - Epoch 1/20, Batch 800/871, Loss: 0.0471\n",
      "2025-10-07 05:20:13,729 - INFO - Epoch 1/20, Batch 810/871, Loss: 0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:20:24,223] [INFO] [Worker ID: 8259]: Processing video 4900/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:20:43,166 - INFO - Epoch 1/20, Batch 820/871, Loss: 0.0513\n",
      "2025-10-07 05:21:08,523 - INFO - Epoch 1/20, Batch 830/871, Loss: 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:21:10,046] [INFO] [Worker ID: 8259]: Processing video 5000/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:21:36,186 - INFO - Epoch 1/20, Batch 840/871, Loss: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:21:59,340] [INFO] [Worker ID: 8192]: Processing video 5100/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:22:14,386 - INFO - Epoch 1/20, Batch 850/871, Loss: 0.0509\n",
      "2025-10-07 05:22:37,561 - INFO - Epoch 1/20, Batch 860/871, Loss: 0.0482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-07 05:22:46,454] [INFO] [Worker ID: 8192]: Processing video 5200/5223...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-07 05:23:08,512 - INFO - Epoch 1/20, Batch 870/871, Loss: 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7887/2150798169.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "[2025-10-07 05:24:03,283] [INFO] [Worker ID: 92901]: Processing video 100/1306...\n",
      "[2025-10-07 05:24:51,689] [INFO] [Worker ID: 92901]: Processing video 200/1306...\n",
      "[2025-10-07 05:25:36,940] [INFO] [Worker ID: 92834]: Processing video 300/1306...\n",
      "[2025-10-07 05:26:23,815] [INFO] [Worker ID: 92703]: Processing video 400/1306...\n",
      "[2025-10-07 05:27:14,403] [INFO] [Worker ID: 92834]: Processing video 500/1306...\n",
      "[2025-10-07 05:28:01,228] [INFO] [Worker ID: 92767]: Processing video 600/1306...\n",
      "[2025-10-07 05:28:48,266] [INFO] [Worker ID: 92767]: Processing video 700/1306...\n",
      "[2025-10-07 05:29:36,954] [INFO] [Worker ID: 92834]: Processing video 800/1306...\n",
      "[2025-10-07 05:30:24,279] [INFO] [Worker ID: 92703]: Processing video 900/1306...\n",
      "[2025-10-07 05:31:10,981] [INFO] [Worker ID: 92767]: Processing video 1000/1306...\n",
      "[2025-10-07 05:32:02,140] [INFO] [Worker ID: 92834]: Processing video 1100/1306...\n",
      "[2025-10-07 05:32:48,236] [INFO] [Worker ID: 92767]: Processing video 1200/1306...\n",
      "[2025-10-07 05:33:37,259] [INFO] [Worker ID: 92834]: Processing video 1300/1306...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m model = DeepfakeDetector(num_frames=\u001b[32m15\u001b[39m, backbone=\u001b[33m\"\u001b[39m\u001b[33mefficientnet_b4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mStarting model training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m train_losses, val_losses, train_accs, val_accs = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     91\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Plot the training and validation loss\u001b[39;00m\n\u001b[32m     93\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, pos_weight, num_epochs, learning_rate, patience)\u001b[39m\n\u001b[32m    187\u001b[39m train_acc = train_correct / \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n\u001b[32m    188\u001b[39m val_acc = val_correct / \u001b[38;5;28mlen\u001b[39m(val_loader.dataset)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m auc_score = \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m avg_val_loss = val_loss / \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[32m    192\u001b[39m train_losses.append(train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Deepfake-Detection/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Deepfake-Detection/.venv/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:665\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    663\u001b[39m y_type = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    664\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m y_score = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mmulticlass\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    668\u001b[39m     y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score.shape[\u001b[32m1\u001b[39m] > \u001b[32m2\u001b[39m\n\u001b[32m    669\u001b[39m ):\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr != \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Deepfake-Detection/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Deepfake-Detection/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/Deepfake-Detection/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    \"\"\"Comprehensive model evaluation with multiple metrics including EER\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad(), autocast():\n",
    "        for data, targets in val_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            targets = targets.unsqueeze(1)\n",
    "\n",
    "            outputs = model(data)  # Raw logits\n",
    "\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate comprehensive metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    auc_score = roc_auc_score(all_targets, all_probabilities)\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        all_targets, all_probabilities\n",
    "    )\n",
    "\n",
    "    # Equal Error Rate (EER) calculation\n",
    "    fpr, tpr, thresh = roc_curve(all_targets, all_probabilities)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresh[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "    if np.all(fpr == 0) or np.all(fnr == 0):\n",
    "        eer = 0.5  # Fallback for degenerate cases\n",
    "    else:\n",
    "        eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
    "\n",
    "    # Plot evaluation metrics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # ROC Curve\n",
    "    axes[0].plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.3f})\")\n",
    "    axes[0].plot([0, 1], [0, 1], \"k--\")\n",
    "    axes[0].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0].set_title(\"ROC Curve\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    axes[1].plot(recall, precision, label=f\"PR Curve\")\n",
    "    axes[1].set_xlabel(\"Recall\")\n",
    "    axes[1].set_ylabel(\"Precision\")\n",
    "    axes[1].set_title(\"Precision-Recall Curve\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_evaluation_metrics.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Model Evaluation Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc_score:.4f}\")\n",
    "    print(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
    "\n",
    "    return accuracy, auc_score, eer\n",
    "\n",
    "\n",
    "# Main execution code\n",
    "if __name__ == \"__main__\":    \n",
    "    # Explicitly tells CUDA to use the 'spawn' start method\n",
    "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    logging.info(\"Initializing deepfake detection model...\")\n",
    "\n",
    "    # Setup data loaders\n",
    "    train_loader, val_loader, pos_weight = setup_training()\n",
    "\n",
    "    # Initialize model\n",
    "    model = DeepfakeDetector(num_frames=15, backbone=\"efficientnet_b4\")\n",
    "\n",
    "    logging.info(\"Starting model training...\")\n",
    "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, pos_weight, num_epochs=20, patience=5\n",
    "    )\n",
    "    # Plot the training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"loss_curves.png\")\n",
    "    plt.show()\n",
    "\n",
    "    logging.info(\"Evaluating model performance...\")\n",
    "    accuracy, auc_score, eer = evaluate_model(model, val_loader)\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), \"deepfake_detector_model.pth\")\n",
    "    logging.info(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8301318,
     "sourceId": 13105087,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8301820,
     "sourceId": 13105799,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 266035024,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
