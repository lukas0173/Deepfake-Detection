{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a978853e-75b3-412f-a4a9-b8b1e19d7969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Setup logging to display progress and informational messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792d72f-a5b6-4fda-9a7d-fb345d996a20",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "To load the saved model weights, we must first define the model's architecture. This class must be identical to the `DeepfakeDetector` class used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfa5319-34c6-4789-b816-8e5615cd1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_frames=20, backbone=\"efficientnet_b4\", dropout_rate=0.6):\n",
    "        super(DeepfakeDetector, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        # CNN Backbone for feature extraction\n",
    "        if backbone == \"efficientnet_b4\":\n",
    "            weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.efficientnet_b4(weights=weights)\n",
    "            self.backbone.classifier = nn.Identity()  # Remove final classifier\n",
    "            feature_dim = 1792\n",
    "        elif backbone == \"resnet50\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.resnet50(weights=weights)\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            feature_dim = 2048\n",
    "\n",
    "        # Temporal processing layers (LSTM)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=512,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism to focus on important frames\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # 512 * 2 for bidirectional LSTM\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_frames, C, H, W = x.shape\n",
    "        x = x.view(batch_size * num_frames, C, H, W)\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size, num_frames, -1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = (lstm_out * attention_weights).sum(dim=1)\n",
    "\n",
    "        output = self.classifier(context_vector)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb74e28-bf2b-449c-a6a7-322c92c62f02",
   "metadata": {},
   "source": [
    "# Load Model and set up for inference\n",
    "This section handles loading the pre-trained model and preparing all necessary components for the inference process, including the face detector (MTCNN) and image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f602a94-ec9c-4f26-948f-ea5505c20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_dependencies():\n",
    "    \"\"\"\n",
    "    Initializes the model, loads weights, and sets up dependencies.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    MODEL_PATH = (\n",
    "        \"models/best_deepfake_detector_model.pth\"  # IMPORTANT: Update this path\n",
    "    )\n",
    "    NUM_FRAMES = 15  # Must match the model's training configuration\n",
    "    IMAGE_SIZE = 224\n",
    "\n",
    "    # Set device (GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = DeepfakeDetector(num_frames=NUM_FRAMES, backbone=\"efficientnet_b4\").to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Load the trained weights\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        logging.info(f\"Successfully loaded model weights from {MODEL_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\n",
    "            f\"Model file not found at {MODEL_PATH}. Please update the MODEL_PATH variable.\"\n",
    "        )\n",
    "        return None, None, None, None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize MTCNN for face detection\n",
    "    mtcnn = MTCNN(\n",
    "        image_size=IMAGE_SIZE,\n",
    "        margin=20,\n",
    "        post_process=False,\n",
    "        device=device,\n",
    "        select_largest=True,  # Handle multiple faces per frame,\n",
    "        selection_method=\"probability\",  # Select the face with the highest confidence\n",
    "    )\n",
    "    logging.info(\"MTCNN face detector initialized.\")\n",
    "\n",
    "    # Define validation transforms (must match those used during training)\n",
    "    val_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model, mtcnn, val_transforms, device, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796cd75-d1c9-4cb6-ac35-761ef0a19aca",
   "metadata": {},
   "source": [
    "# Video inference and bounding box visualization\n",
    "This function performs the core task: processing a video frame by frame, detecting faces, running the deepfake prediction, and drawing bounding boxes with the results on each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec79c3ab-46da-4065-b7ab-10311e0332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_deepfake_in_video(\n",
    "    video_path, output_path, model, mtcnn, transforms, device, image_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a video to detect deepfakes, draws bounding boxes, and saves the output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        logging.error(f\"Video not found at {video_path}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Starting deepfake detection on {video_path}\")\n",
    "\n",
    "    # Create directories for frame outputs\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    frames_output_dir = os.path.join(\n",
    "        output_dir, \"detected_faces\", datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    )\n",
    "    os.makedirs(frames_output_dir, exist_ok=True)\n",
    "\n",
    "    # List to hold prediction data for the CSV\n",
    "    prediction_data = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object to save the output video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(f\"{output_path}/output_video.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    all_probs = []  # A list to store all frame probabilities\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert from BGR (OpenCV) to RGB for PIL and model\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Detect faces and their bounding boxes\n",
    "            boxes, probs = mtcnn.detect(pil_image)\n",
    "\n",
    "            if boxes is not None:\n",
    "                for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
    "                    if prob < 0.95:  # Only process faces with >95% confidence\n",
    "                        continue\n",
    "\n",
    "                    x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "                    # Extract face using bounding box\n",
    "                    face = pil_image.crop((x1, y1, x2, y2))\n",
    "\n",
    "                    # Preprocess the face\n",
    "                    face_resized = face.resize((image_size, image_size))\n",
    "                    face_tensor = transforms(face_resized)\n",
    "\n",
    "                    # The model expects a sequence of frames. We replicate the single face\n",
    "                    # tensor to match the required input shape [1, num_frames, C, H, W].\n",
    "                    input_tensor = (\n",
    "                        face_tensor.unsqueeze(0)\n",
    "                        .repeat(model.num_frames, 1, 1, 1)\n",
    "                        .unsqueeze(0)\n",
    "                        .to(device)\n",
    "                    )\n",
    "\n",
    "                    # Perform prediction\n",
    "                    prediction = model(input_tensor)\n",
    "                    prediction_prob = torch.sigmoid(prediction).item()\n",
    "\n",
    "                    # Append probability to list\n",
    "                    all_probs.append(prediction_prob)\n",
    "\n",
    "                    # Save face and log data\n",
    "                    face_filename = f\"frame{frame_count}_face{i}.jpg\"\n",
    "                    face_save_path = os.path.join(frames_output_dir, face_filename)\n",
    "                    face.save(face_save_path)\n",
    "\n",
    "                    prediction_data.append(\n",
    "                        {\n",
    "                            \"frame_number\": frame_count,\n",
    "                            \"face_index\": i,\n",
    "                            \"bounding_box\": f\"({int(box[0])},{int(box[1])},{int(box[2])},{int(box[3])})\",\n",
    "                            \"prediction_prob\": prediction_prob,\n",
    "                            \"label\": \"FAKE\" if prediction_prob > 0.5 else \"REAL\",\n",
    "                            \"face_image_path\": face_save_path,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Determine label and color for the bounding box\n",
    "                    if prob > 0.5:\n",
    "                        label = f\"FAKE: {prob:.2%}\"\n",
    "                        color = (0, 0, 255)  # Red for Fake\n",
    "                    else:\n",
    "                        label = f\"REAL: {1-prob:.2%}\"\n",
    "                        color = (0, 255, 0)  # Green for Real\n",
    "\n",
    "                    # Draw bounding box and label on the original frame\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        label,\n",
    "                        (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7,\n",
    "                        color,\n",
    "                        2,\n",
    "                    )\n",
    "\n",
    "            out.write(frame)\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                logging.info(f\"Processed {frame_count} frames...\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    if prediction_data:\n",
    "        df = pd.DataFrame(prediction_data)\n",
    "        csv_output_path = os.path.join(output_dir, \"prediction_results.csv\")\n",
    "        df.to_csv(csv_output_path, index=False)\n",
    "        logging.info(f\"Prediction results saved to {csv_output_path}\")\n",
    "\n",
    "    if all_probs:\n",
    "        avg_prob = np.mean(all_probs)\n",
    "        if avg_prob > 0.5:\n",
    "            final_verdict = \"FAKE\"\n",
    "            confidence = avg_prob\n",
    "        else:\n",
    "            final_verdict = \"REAL\"\n",
    "            confidence = 1 - avg_prob\n",
    "        logging.info(f\"--- Video Analysis Complete ---\")\n",
    "        logging.info(f\"Final Verdict: The video is likely {final_verdict}\")\n",
    "        logging.info(f\"Average Confidence: {confidence:.2%}\")\n",
    "    else:\n",
    "        logging.info(\"No faces were confidently detected in the video.\")\n",
    "\n",
    "    logging.info(f\"Detection complete. Output video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d7768-c2eb-47e8-9d48-727ed7c26219",
   "metadata": {},
   "source": [
    "# Main execution block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59f386e-92bd-497d-8376-f958546cfc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 13:12:05,356 - INFO - Using device: cuda\n",
      "2025-10-09 13:12:05,933 - INFO - Successfully loaded model weights from models/best_deepfake_detector_model.pth\n",
      "2025-10-09 13:12:05,946 - INFO - MTCNN face detector initialized.\n",
      "2025-10-09 13:12:05,946 - INFO - Starting deepfake detection on ./id59_id61_0006.mp4\n",
      "2025-10-09 13:12:09,571 - INFO - Processed 100 frames...\n",
      "2025-10-09 13:12:12,871 - INFO - Processed 200 frames...\n",
      "2025-10-09 13:12:16,272 - INFO - Processed 300 frames...\n",
      "2025-10-09 13:12:19,829 - INFO - Processed 400 frames...\n",
      "2025-10-09 13:12:22,107 - INFO - Prediction results saved to prediction_results.csv\n",
      "2025-10-09 13:12:22,108 - INFO - --- Video Analysis Complete ---\n",
      "2025-10-09 13:12:22,108 - INFO - Final Verdict: The video is likely FAKE\n",
      "2025-10-09 13:12:22,109 - INFO - Average Confidence: 52.33%\n",
      "2025-10-09 13:12:22,109 - INFO - Detection complete. Output video saved to .\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- User Configuration ---\n",
    "\n",
    "    INPUT_VIDEO_PATH = \"./id59_id61_0006.mp4\"\n",
    "    OUTPUT_PATH = \".\"\n",
    "\n",
    "    # Load model and dependencies\n",
    "    model, mtcnn, val_transforms, device, image_size = load_model_and_dependencies()\n",
    "\n",
    "    if model:\n",
    "        # Create output directory if it doesn't exist\n",
    "        output_dir = os.path.dirname(OUTPUT_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # Run the deepfake detection process\n",
    "        detect_deepfake_in_video(\n",
    "            video_path=INPUT_VIDEO_PATH,\n",
    "            output_path=OUTPUT_PATH,\n",
    "            model=model,\n",
    "            mtcnn=mtcnn,\n",
    "            transforms=val_transforms,\n",
    "            device=device,\n",
    "            image_size=image_size,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
